{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi-layer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The advantages of Multi-layer Perceptron are:\n",
    "Capability to learn non-linear models.\n",
    "Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "Multi-layer Perceptron is sensitive to feature scaling, \n",
    "so it is highly recommended to scale your data. \n",
    "\n",
    "For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], \n",
    "or standardize it to have mean 0 and variance 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\n",
    "MLP trains on two arrays: \n",
    "    array X of size (n_samples, n_features),\n",
    "    which holds the training samples represented as floating point feature vectors; \n",
    "    and array y of size (n_samples,), which holds the target values (class labels) for the training \n",
    "    samples:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(30, 30, 30, 30, 30, 30),\n",
      "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
      "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "\n",
    "\n",
    "df=pd.read_excel('C:\\\\Users\\\\Dell-pc\\\\Desktop\\\\Data science ML DL python\\\\Practice\\\\Dataset\\\\data set important\\\\titanic.xls')\n",
    "df.head()\n",
    "\n",
    "for c in df.columns:\n",
    "    df[c]=df[c].apply(str)\n",
    "    le=preprocessing.LabelEncoder().fit(df[c])\n",
    "    df[c] =le.transform(df[c])\n",
    "    \n",
    "\n",
    "y=df['survived']\n",
    "x=df.loc[0:, df.columns != 'survived'] \n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30))\n",
    "\n",
    "#where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model.\n",
    "#There are many ways to choose these numbers, but for simplicity we will choose 3 layers with the \n",
    "#same number of neurons as there are features in our data set:\n",
    "\n",
    "#Now that the model has been made we can fit the training data to our model,\n",
    "#remember that this data has already been processed and scaled:\n",
    "print(mlp)\n",
    "mlp.fit(x,y)\n",
    "predictions = mlp.predict(x)\n",
    "\n",
    "#Now we can use SciKit-Learn's built in metrics such as a classification report and confusion matrix \n",
    "#to evaluate how well our model performed:\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y,predictions))\n",
    "\n",
    "\n",
    "print(classification_report(y,predictions))\n",
    "\n",
    "\n",
    "#However, if you do want to extract the MLP weights and biases after training your model,\n",
    "#you use its public attributes coefs_ and intercepts_.\n",
    "\n",
    "#coefs_ is a list of weight matrices, where weight matrix at index i \n",
    "#represents the weights between layer i and layer i+1.\n",
    "\n",
    "print(len(mlp.coefs_[0]))\n",
    "print((mlp.coefs_[0]))\n",
    "\n",
    "#intercepts_ is a list of bias vectors, where the vector at index i \n",
    "#represents the bias values added to layer i+1.\n",
    "\n",
    "print ((mlp.intercepts_[0]))\n",
    "print (len(mlp.intercepts_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no\n",
    "\n",
    "activation function in the output layer, which can also be seen as using the identity function as\n",
    "\n",
    "activation function. Therefore, it uses the square error as the loss function, and the output is a set\n",
    "of continuous values.\n",
    "MLPRegressor also supports multi-output regression, in which a sample can have more than one target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...,    0    0    0]\n",
      " [   0    0    0 ...,    0    0    0]\n",
      " [   0    0    0 ...,    0    0    0]\n",
      " ..., \n",
      " [   0    0    0 ..., 1665    0    0]\n",
      " [   0    0    0 ...,  544    0    0]\n",
      " [   0    0    0 ...,    0    0    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       1922       0.00      0.00      0.00         6\n",
      "       1924       0.00      0.00      0.00         5\n",
      "       1925       0.00      0.00      0.00         7\n",
      "       1926       0.00      0.00      0.00        19\n",
      "       1927       0.00      0.00      0.00        42\n",
      "       1928       0.00      0.00      0.00        52\n",
      "       1929       0.00      0.00      0.00        93\n",
      "       1930       0.00      0.00      0.00        40\n",
      "       1931       0.00      0.00      0.00        35\n",
      "       1932       0.00      0.00      0.00        11\n",
      "       1933       0.00      0.00      0.00         6\n",
      "       1934       0.00      0.00      0.00        29\n",
      "       1935       0.00      0.00      0.00        24\n",
      "       1936       0.00      0.00      0.00        25\n",
      "       1937       0.00      0.00      0.00        28\n",
      "       1938       0.00      0.00      0.00        19\n",
      "       1939       0.00      0.00      0.00        35\n",
      "       1940       0.00      0.00      0.00        52\n",
      "       1941       0.00      0.00      0.00        32\n",
      "       1942       0.00      0.00      0.00        24\n",
      "       1943       0.00      0.00      0.00        14\n",
      "       1944       0.00      0.00      0.00        15\n",
      "       1945       0.00      0.00      0.00        30\n",
      "       1946       0.00      0.00      0.00        29\n",
      "       1947       0.00      0.00      0.00        57\n",
      "       1948       0.00      0.00      0.00        43\n",
      "       1949       0.00      0.00      0.00        60\n",
      "       1950       0.00      0.00      0.00        83\n",
      "       1951       0.00      0.00      0.00        74\n",
      "       1952       0.00      0.00      0.00        77\n",
      "       1953       0.00      0.00      0.00       133\n",
      "       1954       0.00      0.00      0.00       123\n",
      "       1955       0.00      0.00      0.00       275\n",
      "       1956       0.06      0.01      0.02       565\n",
      "       1957       0.08      0.07      0.08       597\n",
      "       1958       0.00      0.00      0.00       583\n",
      "       1959       0.00      0.00      0.00       592\n",
      "       1960       0.00      0.00      0.00       424\n",
      "       1961       0.00      0.00      0.00       571\n",
      "       1962       0.00      0.00      0.00       605\n",
      "       1963       0.00      0.00      0.00       902\n",
      "       1964       0.00      0.00      0.00       945\n",
      "       1965       0.00      0.00      0.00      1120\n",
      "       1966       0.10      0.02      0.03      1377\n",
      "       1967       0.06      0.10      0.08      1718\n",
      "       1968       0.33      0.00      0.00      1867\n",
      "       1969       0.07      0.04      0.05      2210\n",
      "       1970       0.04      0.03      0.03      2349\n",
      "       1971       0.00      0.00      0.00      2131\n",
      "       1972       0.00      0.00      0.00      2288\n",
      "       1973       0.06      0.03      0.04      2596\n",
      "       1974       0.00      0.00      0.00      2184\n",
      "       1975       0.08      0.06      0.07      2482\n",
      "       1976       0.00      0.00      0.00      2179\n",
      "       1977       0.00      0.00      0.00      2502\n",
      "       1978       0.07      0.14      0.09      2926\n",
      "       1979       0.06      0.00      0.01      3108\n",
      "       1980       0.09      0.06      0.07      3101\n",
      "       1981       0.09      0.01      0.02      3162\n",
      "       1982       0.06      0.09      0.07      3597\n",
      "       1983       0.07      0.04      0.05      3386\n",
      "       1984       0.06      0.03      0.04      3368\n",
      "       1985       0.10      0.07      0.08      3578\n",
      "       1986       0.07      0.07      0.07      4219\n",
      "       1987       0.07      0.04      0.05      5122\n",
      "       1988       0.09      0.17      0.12      5611\n",
      "       1989       0.10      0.10      0.10      6670\n",
      "       1990       0.07      0.03      0.04      7256\n",
      "       1991       0.07      0.09      0.08      8647\n",
      "       1992       0.06      0.01      0.01      9543\n",
      "       1993       0.07      0.22      0.10     10525\n",
      "       1994       0.05      0.06      0.05     12121\n",
      "       1995       0.08      0.02      0.03     13257\n",
      "       1996       0.06      0.11      0.08     14130\n",
      "       1997       0.09      0.01      0.01     15182\n",
      "       1998       0.06      0.00      0.00     15814\n",
      "       1999       0.09      0.00      0.00     18238\n",
      "       2000       0.07      0.07      0.07     19285\n",
      "       2001       0.07      0.00      0.01     21589\n",
      "       2002       0.09      0.03      0.05     23451\n",
      "       2003       0.08      0.00      0.00     27382\n",
      "       2004       0.05      0.00      0.00     29607\n",
      "       2005       0.08      0.21      0.12     34952\n",
      "       2006       0.10      0.02      0.04     37534\n",
      "       2007       0.11      0.52      0.18     39404\n",
      "       2008       0.16      0.18      0.17     34760\n",
      "       2009       0.19      0.05      0.08     31038\n",
      "       2010       0.00      0.00      0.00      9396\n",
      "       2011       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.09      0.10      0.06    515344\n",
      "\n",
      "90\n",
      "[[  3.99437068e-315  -3.27563978e-315   4.01772842e-315 ...,\n",
      "    1.12489158e-315  -3.22958009e-315  -9.51768408e-002]\n",
      " [  8.53431027e-316  -3.03678576e-315  -3.39625508e-315 ...,\n",
      "   -7.20619240e-316  -3.40721692e-315  -1.30596950e+000]\n",
      " [ -1.92076478e-316  -3.48833802e-315  -2.10795628e-315 ...,\n",
      "   -1.09648641e-315  -3.09670255e-315  -3.35999251e-001]\n",
      " ..., \n",
      " [ -1.37607816e-315  -3.26392163e-315   1.27797532e-315 ...,\n",
      "    3.05138620e-315  -3.09730548e-315  -1.22441145e-001]\n",
      " [  3.19895319e-315  -3.04873640e-315  -6.31050252e-316 ...,\n",
      "    2.26340492e-315  -3.05438360e-315  -8.81099290e-002]\n",
      " [  3.24778363e-315  -3.41566988e-315   3.42905290e-315 ...,\n",
      "    1.57513540e-315  -3.42540313e-315   8.89097638e-002]]\n",
      "[-0.15787447 -0.00201457 -0.03455901 -0.09901588 -0.06117113 -0.01466537\n",
      " -0.0783337  -0.02735666  0.12880915 -0.22366198 -0.05964061 -0.02870705\n",
      "  0.05875911 -0.0044562  -0.20896378 -0.15856833 -0.20313528  0.22098539\n",
      " -0.00295535 -0.05303538 -0.0095169  -0.0210423  -0.04828518  0.12561881\n",
      " -0.0403578  -0.19617678  0.26579775 -0.18047587 -0.00993818  0.23273448]\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "\n",
    "\n",
    "df=pd.read_csv('C:\\\\Users\\\\Dell-pc\\\\Desktop\\\\Data science ML DL\\\\Practice\\\\Dataset\\\\song\\\\songs.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "y=df['2001']\n",
    "x=df.loc[0:, df.columns != '2001']\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "x= scaler.fit_transform(x)\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(30,30,30,30,30,30))\n",
    "\n",
    "#where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model.\n",
    "#There are many ways to choose these numbers, but for simplicity we will choose 3 layers with the \n",
    "#same number of neurons as there are features in our data set:\n",
    "\n",
    "#Now that the model has been made we can fit the training data to our model,\n",
    "#remember that this data has already been processed and scaled:\n",
    "\n",
    "mlp.fit(x,y)\n",
    "predictions = mlp.predict(x)\n",
    "\n",
    "#Now we can use SciKit-Learn's built in metrics such as a classification report and confusion matrix \n",
    "#to evaluate how well our model performed:\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y,predictions))\n",
    "\n",
    "\n",
    "print(classification_report(y,predictions))\n",
    "\n",
    "\n",
    "#However, if you do want to extract the MLP weights and biases after training your model,\n",
    "#you use its public attributes coefs_ and intercepts_.\n",
    "\n",
    "#coefs_ is a list of weight matrices, where weight matrix at index i \n",
    "#represents the weights between layer i and layer i+1.\n",
    "\n",
    "print(len(mlp.coefs_[0]))\n",
    "print((mlp.coefs_[0]))\n",
    "\n",
    "#intercepts_ is a list of bias vectors, where the vector at index i \n",
    "#represents the bias values added to layer i+1.\n",
    "\n",
    "print ((mlp.intercepts_[0]))\n",
    "print (len(mlp.intercepts_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Both MLPRegressor and MLPClassifier use parameter alpha for regularization (L2 regularization) term \n",
    "which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot\n",
    "displays varying decision function with value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.43696970e-05,   9.99929394e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.neural_network import MLPRegressor\n",
    "\n",
    ">>> X = [[0., 0.], [1., 1.]]\n",
    ">>> y = [0, 1]\n",
    ">>> clf = MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "...                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "...\n",
    ">>> clf.fit(X, y)                         \n",
    "y1=clf.predict(X)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Restricted Boltzmann Machine (RBM).\n",
    "\n",
    "A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].\n",
    "4+\n",
    "The time complexity of this implementation is O(d ** 2) assuming d ~ n_features ~ n_components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
       "       random_state=None, verbose=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.neural_network import BernoulliRBM\n",
    ">>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    ">>> model = BernoulliRBM(n_components=2)\n",
    ">>> model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics,Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. \n",
    "Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with \n",
    "\n",
    "\n",
    "where n is the learning rate which controls the step-size in the parameter space search. \n",
    "Loss is the loss function used for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Suppose there are n training samples, m features, k hidden layers, each containing h \n",
    "neurons - for simplicity, and o output neurons. The time complexity of backpropagation is \n",
    "O(n m h^K o i), where i is the number of iterations. \n",
    "Since backpropagation has a high time complexity, it is advisable to start with smaller number of\n",
    "hidden neurons and few hidden layers for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> [coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
